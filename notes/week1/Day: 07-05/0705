I fixed my dataset issue. Unsloth asked a struct for fine-tuning; I was giving a string.

I tried a second one and I received a message telling me that once I loaded my base LLaMA in 4-bit mode, all of its weights are frozen, but I need to add a small, trainable “adapter” (e.g., LoRA) on top before you can fine-tune.

So I decided to take another model for fine-tuning (Llama-3.2-1B-Instruct-bnb-4bit).
Training seems long on my 7.6 GB GPU but not impossible.
For a dataset of 10,000 lines of (‘user’ → ‘response’):

    started at 09:30

    ended at 10:00

It worked—YEEEEEEEYYYY. I didn’t save... I didn’t save the model.

So I proceeded with another fine-tuning, saving the model at the end:

    started at 10:10

    ended at 10:40

I saw this: Trainable parameters = 851,968 / 1,000,000,000 (0.09% trained).
I need to investigate.

Ok, I needed to understand better the concept of fine-tuning.

    Fine-tuning is a method to modify a model without recalculating all the neurons in the network.

    Since the neurons are stored in a matrix, we can use matrix multiplication to change an area of the matrix (see illustration FT).

    Because a neural network uses a series of neurons to perform its calculations, changing one or two can already lead to the expected result without recalculating the rest.

    So we multiply a tiny matrix with the model’s weight matrix to apply fine-tuning:

    NewModel = BaseModel * TrainingMatrix

    We usually use a pretrained model because it allows us to focus only on our new training; but it may also be possible to fine-tune an already trained model (researchers).

Now LoRA:

    The idea is to optimize fine-tuning, since multiplying high-dimensional matrices can be costly.

    Instead, we decompose the TrainingMatrix into two smaller matrices:

    TrainingMatrix = Rank1 * Rank2
    NewModel      = BaseModel * Rank1 * Rank2

    In practice, the trainable parameters (Rank1 and Rank2) are chosen from the start and can yield high-quality results even at low parameter counts.

After investigation and reflection, my dataset full of

Where is 'name'? → In 'country'!

seems to lead to good results because the task is simple. Maybe a model with twice as many parameters would train faster?

I proceeded to fine-tune Llama-3.2-1B-Instruct-bnb-4bit with rank = 16 instead of 8. I expect at least 1 hour of training (2× the time of the previous fine-tuning), but it may vary.

    started at 16:04

    ended myself

We will do that tomorrow.
