I decided to take everything from zero.
https://www.youtube.com/watch?v=dMY3dBLojTk
https://github.com/technovangelist/videoprojects/tree/main/2025-01-24-unslothfinetune

Found a cool video to get refamiliarized with the framework.

First things first: Unsloth is not a fine-tuning tool; it is an optimizer.

I tried to create a program to test the model. I failed.

I dived into the frameworks for fine-tuning and found two others: MLX and Axolotl (which I already knew).

MLX is specific to Apple: https://www.youtube.com/watch?v=BCfCdTp-fdM
So no MLX, which is sad because it seems really performant.

I will look at Axolotl: https://www.youtube.com/watch?v=lj44Bt9UxYQ
Sounds really simple (this is a no-code framework) and can work with multiple GPUs.

After having written the YAML for the fine-tuning, it’s really hard to make it work.

I took this as an example: https://github.com/axolotl-ai-cloud/axolotl/blob/main/examples/mistral/config.yml
And asked my AI ChatGPT friend to help me. But for now we didn’t succeed.
Axloth despite being simple to use IS NOT simple to install. Python is the worst programming langage that ever exist.

------------------------------UPDATE---------------------------------------------------


My intership manager just gave me instructions for my task. I need to build a question-decomposer model.

For the question: “When did WW2 start and why? Can you explain to me?”
The model should answer somthing like this:

{question: "When did WW2 start"}{question: "Why did WW2 start"}

I was given the idea to use Qwent to create datasets. I will investigate now. I am stopping Axolotl. I am making the dataset a priority.

I may use an LLM ( like Qwent ) to help me create the dataset. I could, but I think I will look at the existing datasets on Hugging Face first. I may get ideas.
