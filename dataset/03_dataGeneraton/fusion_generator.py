import os
import json
from itertools import combinations
from ollama import Client

def generate_fr_dataset(
    questions,
    output_path,
    model_name="llama3.2:latest",
    use_gpu=True,
    max_attempts=10
):
    # S'assurer que le dossier existe
    folder = os.path.dirname(output_path)
    if folder:
        os.makedirs(folder, exist_ok=True)

    client = Client()
    print(f"ğŸ’» Client Ollama initialisÃ© avec modÃ¨le '{model_name}' (GPU={'oui' if use_gpu else 'non'})")

    # Configuration des prompts
    sys_complex = (
        "Tu es un assistant qui **ne renvoie que** la question complexe fusionnant les deux questions,"
        " sans explication, sans balises, sans guillemets superflus,"  
        " sans inventer aucune information."
    )
    sys_rephrase = (
        "Tu es un assistant qui **ne renvoie que** la question reformulÃ©e,"
        " sans explication, sans balises, sans guillemets superflus."
    )

    basic_variants = [
        ("simple", sys_complex,
         lambda q1, q2: (
             f"Fusionne ces deux questions en une seule question volontairement trÃ¨s simple:\n"
             f"- {q1}\n"
             f"- {q2}"
         )
        )
    ]

    advanced_variants = [
        ("fluide",     sys_rephrase, lambda q: f"Reformule cette question de maniÃ¨re fluide et claire : Â« {q} Â»"),
        ("complexe2",  sys_rephrase, lambda q: f"Reformule cette question de maniÃ¨re volontairement complexe : Â« {q} Â»"),
        ("courte",     sys_rephrase, lambda q: f"Reformule cette question en version courte et concise : Â« {q} Â»"),
        #("ouverte",    sys_rephrase, lambda q: f"Transforme cette question en question ouverte pour lancer la discussion : Â« {q} Â»"),
        #("academique", sys_rephrase, lambda q: f"RÃ©dige cette question dans un style acadÃ©mique et formel : Â« {q} Â»"),
        #("fermee",     sys_rephrase, lambda q: f"Reformule en question fermÃ©e (oui/non) : Â« {q} Â»"),
        #("niveau_ce2", sys_rephrase, lambda q: f"Adapte cette question pour un Ã©lÃ¨ve de CE2 : Â« {q} Â»"),
        #("suivi",      sys_rephrase, lambda q: f"Ã€ partir de Â« {q} Â», formule une question de suivi pour approfondir le sujet"),
        #("passive",    sys_rephrase, lambda q: f"Reformule cette question Ã  la voix passive : Â« {q} Â»"),
        #("comparative",sys_rephrase, lambda q: f"Transforme cette question pour comparer deux entitÃ©s : Â« {q} Â»")
    ]

    def get_ollama_options():
        opts = {"temperature": 0.2, "top_p": 0.9, "num_predict": 100}
        if use_gpu:
            opts["device"] = "cuda"
        return opts

    def generate_unique_question(system_prompt, user_prompt, seen):
        for attempt in range(1, max_attempts + 1):
            print(f"  âš™ï¸ Tentative {attempt}/{max_attempts} pour: '{user_prompt[:50]}...'" )
            resp = client.chat(
                model=model_name,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user",   "content": user_prompt}
                ],
                options=get_ollama_options()
            )
            text = resp.message.content.strip()
            if text and text not in seen:
                print(f"    âœ… Obtention: '{text}'")
                return text
            print("    âš ï¸ Doublon ou vide, nouvel essaiâ€¦")
        print(f"    âŒ Ã‰chec aprÃ¨s {max_attempts} tentatives pour: '{user_prompt}'")
        return None

    dataset = []
    pairs = list(combinations(questions, 2))
    total = len(pairs)
    print(f"ğŸ”„ DÃ©marrage de la gÃ©nÃ©ration pour {total} paires...")

    count = 0
    for idx, (q1, q2) in enumerate(pairs, start=1):
        print("\n" + "="*60)
        print(f" ğŸ¤– Exemple {idx}/{total}  RÃ©ponses gÃ©nÃ©rÃ©es: {count}")
        print("="*60)
        print(f"Question 1: {q1}\nQuestion 2: {q2}\n")

        seen = set()
        base_answer = f"{q1} {q2}"

        # Variante simple
        name, sys_prompt, tpl = basic_variants[0]
        print(f"ğŸ› ï¸ GÃ©nÃ©ration ({name})â€¦")
        user_prompt = tpl(q1, q2)
        simple_q = generate_unique_question(sys_prompt, user_prompt, seen)
        if not simple_q:
            print(f"âš ï¸ Ã‰chec de la variante simple, passage Ã  l'exemple suivant.")
            continue
        print(f"âœ… {name.capitalize()}: {simple_q}\n")
        seen.add(simple_q)
        dataset.append({"question": simple_q, "answer": base_answer})
        count += 1

        # Variantes avancÃ©es
        for name, sys_prompt, tpl in advanced_variants:
            print(f"ğŸ› ï¸ Reformulation ({name})â€¦")
            user_prompt = tpl(simple_q)
            resp = generate_unique_question(sys_prompt, user_prompt, seen)
            if not resp:
                print(f"âš ï¸ Ã‰chec de la variante avancÃ©e '{name}'")
                continue
            print(f"âœ… {name.capitalize()}: {resp}\n")
            seen.add(resp)
            dataset.append({"question": resp, "answer": base_answer})
            count += 1

    # Sauvegarde finale
    print("ğŸ“š Enregistrement du dataset JSONLâ€¦")
    with open(output_path, "w", encoding="utf-8") as f:
        for entry in dataset:
            f.write(json.dumps(entry, ensure_ascii=False) + "\n")
    print(f"ğŸ‰ TerminÃ© : {len(dataset)} entrÃ©es gÃ©nÃ©rÃ©es dans '{output_path}'.")
