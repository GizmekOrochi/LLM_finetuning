import os
import re
import json
from ollama import Client

def slugify(text: str) -> str:
    """
    Transforme une cha√Æne en slug pour nom de fichier :
    remplace les caract√®res non alphanum√©riques par des underscores.
    """
    return re.sub(r"[^\w]+", "_", text).strip("_")

def generate_fr_base_questions(
    topic: str,
    num_questions: int,
    output_dir: str = "question_groups",
    model_name: str = "llama3.2:latest",
    use_gpu: bool = True,
    max_attempts: int = 3
):
    """
    G√©n√®re un fichier JSONL de questions simples en fran√ßais sur un th√®me donn√©,
    une question √† la fois, et le place dans `output_dir`.
    Affiche chaque question d√®s qu'elle est g√©n√©r√©e.
    """
    os.makedirs(output_dir, exist_ok=True)
    slug = slugify(topic)
    output_path = os.path.join(output_dir, f"{slug}.jsonl")
    if os.path.exists(output_path):
        os.remove(output_path)

    client = Client()
    print(f"üíª Client Ollama initialis√© avec '{model_name}' (GPU={'oui' if use_gpu else 'non'})\n")

    sys_prompt = (
        "Tu es un assistant expert en g√©n√©ration de questions simples. "
        "Tu ne renvoies **que** la question, sans guillemets superflus, "
        "sans num√©ros ni pr√©fixes, juste le texte de la question."
    )

    options = {"temperature": 0.7, "top_p": 0.9, "num_predict": 100}
    if use_gpu:
        options["device"] = "cuda"

    with open(output_path, "w", encoding="utf-8") as fout:
        for i in range(1, num_questions + 1):
            user_prompt = (
                f"G√©n√®re une question simple en fran√ßais sur le th√®me ¬´ {topic} ¬ª "
                f"(question {i} sur {num_questions})."
            )
            question = None

            for attempt in range(1, max_attempts + 1):
                print(f"  ‚öôÔ∏è G√©n√©ration question {i}/{num_questions}, essai {attempt}/{max_attempts}‚Ä¶")
                resp = client.chat(
                    model=model_name,
                    messages=[
                        {"role": "system", "content": sys_prompt},
                        {"role": "user",   "content": user_prompt},
                    ],
                    options=options
                )
                text = resp.message.content.strip()
                if text:
                    question = text
                    break
                print("    ‚ö†Ô∏è Sortie vide, nouvel essai‚Ä¶")

            if question is None:
                raise RuntimeError(f"üõë √âchec apr√®s {max_attempts} tentatives pour la question {i}.")

            print(f"  ‚úÖ Question {i}: {question}")
            fout.write(json.dumps({"question": question}, ensure_ascii=False) + "\n")

    print(f"\nüéâ Termin√© ! {num_questions} questions g√©n√©r√©es dans ¬´ {output_path} ¬ª.")

def run_generate_questions(
    topic: str,
    num_questions: int,
    output_dir: str = "dataset/question_groups",
    model_name: str = "llama3.2:latest",
    use_gpu: bool = True
):
    """
    Wrapper to call generate_fr_base_questions from other files.
    """
    generate_fr_base_questions(topic, num_questions, output_dir, model_name, use_gpu)

if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser(description="Generate French simple questions")
    parser.add_argument("topic", help="Le th√®me des questions")
    parser.add_argument("num_questions", type=int, help="Nombre de questions √† g√©n√©rer")
    parser.add_argument("--output_dir", default="dataset/question_groups", help="R√©pertoire de sortie")
    parser.add_argument("--model_name", default="llama3.2:latest", help="Nom du mod√®le Ollama")
    parser.add_argument("--no_gpu", action="store_true", help="Ne pas utiliser le GPU")
    args = parser.parse_args()
    run_generate_questions(
        topic=args.topic,
        num_questions=args.num_questions,
        output_dir=args.output_dir,
        model_name=args.model_name,
        use_gpu=not args.no_gpu,
    )
