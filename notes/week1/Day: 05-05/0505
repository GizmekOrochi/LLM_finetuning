Ok I need to focus on what is the finetuning becaus I never really successed at training a model. Hopefully Youtube must have greats vidéos on this suject and I can use the online models to help me with the python code which I am a bit rusty on.

RESOURCE – How to finetune: https://www.youtube.com/watch?v=Lt7KrFMcCis

There is what I learned todays:

First: Pre-tuning

    Give the model the ability to predict the next token

Post-training
    Make the model better follow human preference
    Make the model safer

(Creation of reasoning model)
    Data with reward function instead of question–answer

We are going to finetune using QLoRA (method that allows us to finetune efficiently with high-cost GPU).

Using Unsloth, we are using base models:
    Base models are models just after the pre-training phase

RESOURCE – How to create dataset: https://github.com/vossenwout/llm-finetuning-resources/tree/main
	Saving this for later.
After training
    Saving model
    Quantization: “Q4_K8M” is the most popular. “Q4_K8M” or “Q_8” are recommended. Can be done with llama.cpp or Unsloth
    UI: llama.cpp can be used for the interface. RESOURCE: https://github.com/ggml-org/llama.cpp


