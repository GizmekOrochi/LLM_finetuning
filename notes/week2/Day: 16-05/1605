I have a program that generates the dataset. I created a document (dataset/03_dataGeneration) explaining how it works. This code will produce a huge dataset for me.

I wanted to calculate in advance how many lines it will generate, but I think Deepseek or ChatGPT could write that calculation faster.

I’ll generate a dataset covering these topics:

topics = [
    "Préhistoire",
    "Antiquité égyptienne",
    "Antiquité gréco-romaine",
    "Empire romain",
    "Empire byzantin",
    "Haut Moyen Âge",
    "Bas Moyen Âge",
    "Croisades",
    "Renaissance",
    "Réforme protestante",
    "Guerre de Cent Ans",
    "Monarchie absolue",
    "Révolution française",
    "Époque napoléonienne",
    "Révolution industrielle",
    "Première Guerre mondiale",
    "Seconde Guerre mondiale",
    "Guerre froide",
    "Décolonisation",
    "Mondialisation contemporaine"
]

Each topic will yield 50 simple questions, so the expected total number of examples is

20 topics × 50 questions = 1,000 questions per run

If I run the full loop until each topic is complete, that’s

20 topics × 50 questions × 147 runs ≃ 147,000 lines

– let’s say at least 140,000 examples in the final dataset.

I don’t know exactly how long it will take, so I let the program run and checked its progress every 20 minutes:

    Started at 14:30

    16:30 → 30%

    20:00 → 90%

    20:05 → 1% (I think the program restarted itself)

    23:12 → 30%

    01:37 → 90%

    Ended at 01:50

It turned out there was an issue in the code: some of the smaller datasets were generated, but the program got stuck looping on a few topics. I’ll do more tests later.

03:24 update:
Actually, there’s no bug in the generation logic—I just hadn’t instrumented it well. I’ll add more print statements to track progress more accurately. Did it.
